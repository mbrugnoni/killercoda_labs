
In this lab, we're going to show you how easy it is to run a local LLM (Large Language Model). There's many reasons why you'd want to do this, whether it be for AI development, avoiding API cost, privacy, productivity, or just plain fun!

## Objectives

- Install Ollama
- Use Ollama to run the Tinyllama model
- Query the model via interactive CLI
- Query the model via API
- Have fun!

## Prerequisite Skills

A basic understanding of Linux is beneficial.


