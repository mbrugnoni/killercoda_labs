
Great job! Hopefully you find a create way to use a local LLM!

You were able to:
- Install Ollama and download Tinyllama LLM
- Interact with your model interactively
- Send a prompt to your model using the API

## Further Learning

Join the ProLUG (Professional Linux User Group) Discord and keep up to date on new labs: https://discord.gg/hETV36MVyG