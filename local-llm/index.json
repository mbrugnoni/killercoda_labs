{
  "title": "Running a local LLM for Beginners",
  "description": "How you can easily run a local LLM on minimal hardware",
  "details": {
    "intro": {
      "text": "intro.md",
      "background": "setup.sh"
    },
    "steps": [
      {
        "title": "Installing Ollama",
        "text": "step1/text.md",
        "verify": "step1/verify.sh"
      },
      {
        "title": "Run Tinyllama model using Ollama",
        "text": "step2/text.md",
        "verify": "step2/verify.sh"
      },
      {
        "title": "Interact with model through API",
        "text": "step3/text.md",
        "verify": "step3/verify.sh"
      }
    ],
    "finish": {
      "text": "finish.md"
    }
  },
  "backend": {
    "imageid": "ubuntu"
  }
}